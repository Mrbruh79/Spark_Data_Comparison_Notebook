{
          "cells": [
                    {
                              "cell_type": "markdown",
                              "id": "8d7bee9b",
                              "metadata": {},
                              "source": [
                                        "# Data Comparison Notebook\n",
                                        "\n",
                                        "This notebook compares two datasets (tables or S3 paths) and generates row-count, null-rate, distinct-count, date-level, and missing-record diagnostics.\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "633c732b",
                              "metadata": {},
                              "source": [
                                        "## Environment Setup\n",
                                        "\n",
                                        "Use this notebook in either local Spark or Databricks.\n",
                                        "\n",
                                        "### Local Spark\n",
                                        "- Install dependencies: `pyspark`, `pandas`, `boto3`, `openpyxl`.\n",
                                        "- Run with a local Python/Jupyter kernel.\n",
                                        "- Use a local output path such as `./outputs`.\n",
                                        "\n",
                                        "### Databricks or Databricks Connect\n",
                                        "- On Databricks clusters, Spark is already available.\n",
                                        "- For Databricks Connect, configure credentials first (`databricks-connect configure`).\n",
                                        "- For DBFS output, set `output_base_path` to `/dbfs/FileStore/<folder>` or `dbfs:/FileStore/<folder>`.\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 1,
                              "id": "488f2046",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "from dataclasses import dataclass, field\n",
                                        "from datetime import datetime, timedelta\n",
                                        "from typing import Dict, List, Optional, Tuple\n",
                                        "\n",
                                        "import boto3\n",
                                        "import os\n",
                                        "import pandas as pd\n",
                                        "from pyspark.sql import DataFrame, SparkSession\n",
                                        "from pyspark.sql.functions import coalesce, col, count, countDistinct, lit, to_date, when\n",
                                        "from pyspark.storagelevel import StorageLevel\n",
                                        "\n",
                                        "try:\n",
                                        "    # Available when running with Databricks Connect.\n",
                                        "    from databricks.connect.session import DatabricksSession\n",
                                        "except Exception:\n",
                                        "    DatabricksSession = None\n",
                                        "\n",
                                        "\n",
                                        "def get_spark_session() -> SparkSession:\n",
                                        "    \"\"\"Return an active Spark session for local Spark or Databricks Connect.\"\"\"\n",
                                        "    if DatabricksSession is not None:\n",
                                        "        return DatabricksSession.builder.getOrCreate()\n",
                                        "    return SparkSession.builder.getOrCreate()\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "7c470076",
                              "metadata": {},
                              "source": [
                                        "## Configuration Model\n",
                                        "\n",
                                        "`DataComparisonConfig` controls dataset selection, filtering, authentication, and exports.\n",
                                        "\n",
                                        "### Required fields\n",
                                        "- `source_1_table`, `source_2_table`: Source table names or S3 paths.\n",
                                        "- `primary_key`: Join key used for missing-record analysis.\n",
                                        "- `domain_name`: Label used in logs and output filenames.\n",
                                        "\n",
                                        "### Common optional fields\n",
                                        "- `count_key`: Column used for grouped counts (defaults to `primary_key`).\n",
                                        "- `datetime_columns`: One or more datetime/date columns for time-based analysis.\n",
                                        "- `filter_date_start`, `filter_date_end`: Inclusive date window in `YYYYMMDD` format.\n",
                                        "- `partition_column`: Optional S3 partition key for partition-aware reads.\n",
                                        "- `aws_*`: Optional explicit AWS credentials (otherwise default credential chain is used).\n",
                                        "- `output_base_path`: Output directory for Excel/CSV exports.\n",
                                        "- `enable_persist`: Enables Spark persistence where supported.\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 2,
                              "id": "9a17c19b",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "def _is_databricks_runtime() -> bool:\n",
                                        "    \"\"\"Return True when running inside a Databricks runtime.\"\"\"\n",
                                        "    return bool(os.getenv(\"DATABRICKS_RUNTIME_VERSION\") or os.getenv(\"DB_HOME\"))\n",
                                        "\n",
                                        "\n",
                                        "def _default_output_base_path(domain_name: str) -> str:\n",
                                        "    \"\"\"Resolve a default output folder based on execution environment.\"\"\"\n",
                                        "    if _is_databricks_runtime() and os.path.isdir(\"/dbfs\"):\n",
                                        "        return os.path.join(\"/dbfs\", \"FileStore\", domain_name)\n",
                                        "    return os.path.join(\".\", \"outputs\", domain_name)\n",
                                        "\n",
                                        "\n",
                                        "@dataclass\n",
                                        "class DataComparisonConfig:\n",
                                        "    \"\"\"Configuration for comparing two datasets and exporting diagnostics.\"\"\"\n",
                                        "\n",
                                        "    source_1_table: str\n",
                                        "    source_2_table: str\n",
                                        "    primary_key: str\n",
                                        "    domain_name: str\n",
                                        "    count_key: Optional[str] = None\n",
                                        "\n",
                                        "    # Optional date filtering on dataframe columns\n",
                                        "    datetime_columns: Optional[List[str]] = field(default_factory=list)\n",
                                        "    filter_date_start: Optional[str] = None  # format: YYYYMMDD\n",
                                        "    filter_date_end: Optional[str] = None    # format: YYYYMMDD\n",
                                        "    date_format: str = \"%Y%m%d\"\n",
                                        "    skip_date_filter: bool = True\n",
                                        "\n",
                                        "    # Optional partition-aware S3 loading\n",
                                        "    partition_column: Optional[str] = None\n",
                                        "\n",
                                        "    # Output settings\n",
                                        "    output_base_path: Optional[str] = None\n",
                                        "    excel_output: Optional[str] = None\n",
                                        "    csv_output: Optional[str] = None\n",
                                        "\n",
                                        "    # Persist behavior (serverless-safe)\n",
                                        "    enable_persist: bool = True\n",
                                        "\n",
                                        "    # Standard AWS S3 authentication inputs\n",
                                        "    aws_region: str = \"us-east-1\"\n",
                                        "    aws_access_key_id: Optional[str] = None\n",
                                        "    aws_secret_access_key: Optional[str] = None\n",
                                        "    aws_session_token: Optional[str] = None\n",
                                        "\n",
                                        "    def __post_init__(self):\n",
                                        "        \"\"\"Normalize defaults, output paths, and optional datetime settings.\"\"\"\n",
                                        "        if not self.count_key:\n",
                                        "            self.count_key = self.primary_key\n",
                                        "\n",
                                        "        if self.datetime_columns is None:\n",
                                        "            self.datetime_columns = []\n",
                                        "        elif isinstance(self.datetime_columns, str):\n",
                                        "            self.datetime_columns = [self.datetime_columns]\n",
                                        "\n",
                                        "        if self.output_base_path is None:\n",
                                        "            self.output_base_path = _default_output_base_path(self.domain_name)\n",
                                        "        if self.output_base_path.startswith(\"dbfs:\"):\n",
                                        "            self.output_base_path = self.output_base_path.replace(\"dbfs:\", \"/dbfs\", 1)\n",
                                        "        if self.excel_output is None:\n",
                                        "            self.excel_output = f\"{self.domain_name}_comparison.xlsx\"\n",
                                        "        if self.csv_output is None:\n",
                                        "            self.csv_output = f\"{self.domain_name}_comparison.csv\"\n",
                                        "\n",
                                        "        os.makedirs(self.output_base_path, exist_ok=True)\n",
                                        "\n",
                                        "    @staticmethod\n",
                                        "    def infer_datetime_type(column_name: str) -> str:\n",
                                        "        column_lower = column_name.lower()\n",
                                        "        if \"create\" in column_lower and (\"dt\" in column_lower or \"date\" in column_lower):\n",
                                        "            return \"iso\"\n",
                                        "        if any(token in column_lower for token in [\"processdate\", \"process_dt\", \"eventdate\", \"event_date\"]):\n",
                                        "            return \"yyyymmdd\"\n",
                                        "        return \"timestamp\"\n",
                                        "\n",
                                        "    def get_standardized_date_column_name(self, datetime_column: str) -> str:\n",
                                        "        return f\"{datetime_column}_date\"\n",
                                        "\n",
                                        "    @property\n",
                                        "    def excel_path(self) -> str:\n",
                                        "        return os.path.join(self.output_base_path, self.excel_output)\n",
                                        "\n",
                                        "    @property\n",
                                        "    def csv_path(self) -> str:\n",
                                        "        return os.path.join(self.output_base_path, self.csv_output)\n",
                                        "\n",
                                        "\n",
                                        "def create_configs_from_pairs(config_rows: List[Dict]) -> List[DataComparisonConfig]:\n",
                                        "    \"\"\"Build configs from a list of dictionaries for batch execution.\"\"\"\n",
                                        "    return [DataComparisonConfig(**row) for row in config_rows]\n",
                                        "\n",
                                        "\n",
                                        "def _normalize_datetime_columns(value) -> List[str]:\n",
                                        "    \"\"\"Normalize datetime column input into a list of column names.\"\"\"\n",
                                        "    if value is None:\n",
                                        "        return []\n",
                                        "    if isinstance(value, str):\n",
                                        "        return [value]\n",
                                        "    return list(value)\n",
                                        "\n",
                                        "\n",
                                        "def create_configs_from_nested_dict(nested_config: Dict) -> List[DataComparisonConfig]:\n",
                                        "    \"\"\"Create configs from grouped source tables.\n",
                                        "\n",
                                        "    Expected shape:\n",
                                        "    {\n",
                                        "        \"group\": (\n",
                                        "            [source_1_table_desc, ...],\n",
                                        "            [source_2_table_desc, ...],\n",
                                        "        )\n",
                                        "    }\n",
                                        "\n",
                                        "    Each table description can be:\n",
                                        "    - str: table/path\n",
                                        "    - tuple/list: (table, primary_key, count_key, domain_name, datetime_columns)\n",
                                        "    - dict: {table, primary_key, count_key, domain_name, datetime_columns}\n",
                                        "    \"\"\"\n",
                                        "    configs: List[DataComparisonConfig] = []\n",
                                        "\n",
                                        "    for _, pair in nested_config.items():\n",
                                        "        if len(pair) != 2:\n",
                                        "            raise ValueError(\"Each nested config entry must contain (source_1_tables, source_2_tables)\")\n",
                                        "\n",
                                        "        source_1_tables, source_2_tables = pair\n",
                                        "        if len(source_1_tables) != len(source_2_tables):\n",
                                        "            raise ValueError(\"source_1/source_2 table list lengths must match\")\n",
                                        "\n",
                                        "        for source_1_desc, source_2_desc in zip(source_1_tables, source_2_tables):\n",
                                        "            if isinstance(source_1_desc, str):\n",
                                        "                source_1_table = source_1_desc\n",
                                        "                primary_key = \"id\"\n",
                                        "                count_key = \"id\"\n",
                                        "                domain_name = source_1_table.rstrip(\"/\").split(\"/\")[-1]\n",
                                        "                datetime_columns = []\n",
                                        "            elif isinstance(source_1_desc, (tuple, list)):\n",
                                        "                source_1_table = source_1_desc[0]\n",
                                        "                primary_key = source_1_desc[1] if len(source_1_desc) > 1 else \"id\"\n",
                                        "                count_key = source_1_desc[2] if len(source_1_desc) > 2 else primary_key\n",
                                        "                domain_name = source_1_desc[3] if len(source_1_desc) > 3 else source_1_table.rstrip(\"/\").split(\"/\")[-1]\n",
                                        "                datetime_columns = _normalize_datetime_columns(source_1_desc[4]) if len(source_1_desc) > 4 else []\n",
                                        "            elif isinstance(source_1_desc, dict):\n",
                                        "                source_1_table = source_1_desc[\"table\"]\n",
                                        "                primary_key = source_1_desc.get(\"primary_key\", \"id\")\n",
                                        "                count_key = source_1_desc.get(\"count_key\", primary_key)\n",
                                        "                domain_name = source_1_desc.get(\"domain_name\", source_1_table.rstrip(\"/\").split(\"/\")[-1])\n",
                                        "                datetime_columns = _normalize_datetime_columns(source_1_desc.get(\"datetime_columns\", []))\n",
                                        "            else:\n",
                                        "                raise ValueError(f\"Unsupported source_1 descriptor type: {type(source_1_desc)}\")\n",
                                        "\n",
                                        "            if isinstance(source_2_desc, str):\n",
                                        "                source_2_table = source_2_desc\n",
                                        "            elif isinstance(source_2_desc, (tuple, list)):\n",
                                        "                source_2_table = source_2_desc[0]\n",
                                        "            elif isinstance(source_2_desc, dict):\n",
                                        "                source_2_table = source_2_desc[\"table\"]\n",
                                        "            else:\n",
                                        "                raise ValueError(f\"Unsupported source_2 descriptor type: {type(source_2_desc)}\")\n",
                                        "\n",
                                        "            configs.append(\n",
                                        "                DataComparisonConfig(\n",
                                        "                    source_1_table=source_1_table,\n",
                                        "                    source_2_table=source_2_table,\n",
                                        "                    primary_key=primary_key,\n",
                                        "                    count_key=count_key,\n",
                                        "                    domain_name=domain_name,\n",
                                        "                    datetime_columns=datetime_columns,\n",
                                        "                )\n",
                                        "            )\n",
                                        "\n",
                                        "    return configs\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "c7cc6412",
                              "metadata": {},
                              "source": [
                                        "## Analyzer Workflow\n",
                                        "\n",
                                        "`DataComparisonAnalyzer` executes the end-to-end comparison pipeline.\n",
                                        "\n",
                                        "1. Load `source_1` and `source_2` (table or S3 parquet).\n",
                                        "2. Apply optional date filters.\n",
                                        "3. Compare total counts.\n",
                                        "4. Compute per-column null and distinct metrics.\n",
                                        "5. Identify key-level missing records in each source.\n",
                                        "6. Produce optional date-wise and enhanced schema diagnostics.\n",
                                        "7. Export reports to Excel/CSV.\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 3,
                              "id": "b6ef2803",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "class DataComparisonAnalyzer:\n",
                                        "    \"\"\"Run dataset-level comparison, quality checks, and exports.\"\"\"\n",
                                        "\n",
                                        "    def __init__(self, config: DataComparisonConfig):\n",
                                        "        self.config = config\n",
                                        "        self.spark = get_spark_session()\n",
                                        "\n",
                                        "        self.df_source_1: Optional[DataFrame] = None\n",
                                        "        self.df_source_2: Optional[DataFrame] = None\n",
                                        "        self.results: Dict = {}\n",
                                        "\n",
                                        "        self._s3_configured = False\n",
                                        "\n",
                                        "    def _is_serverless(self) -> bool:\n",
                                        "        try:\n",
                                        "            return self.spark.conf.get(\n",
                                        "                \"spark.databricks.clusterUsageTags.serverless\", \"false\"\n",
                                        "            ).lower() == \"true\"\n",
                                        "        except Exception:\n",
                                        "            return False\n",
                                        "\n",
                                        "    def _safe_persist(self, df: DataFrame) -> DataFrame:\n",
                                        "        if not self.config.enable_persist:\n",
                                        "            return df\n",
                                        "        if self._is_serverless():\n",
                                        "            print(\"Persist skipped: Databricks serverless does not support persist().\")\n",
                                        "            enable_persist: bool = True\n",
                                        "            return df\n",
                                        "        try:\n",
                                        "            df.persist(StorageLevel.MEMORY_AND_DISK)\n",
                                        "        except Exception as exc:\n",
                                        "            print(f\"Persist skipped: {exc}\")\n",
                                        "        return df\n",
                                        "\n",
                                        "    def _configure_s3_authentication(self):\n",
                                        "        \"\"\"Configure Spark S3 access using explicit or default AWS credentials.\"\"\"\n",
                                        "        if self._s3_configured:\n",
                                        "            return\n",
                                        "\n",
                                        "        access_key = self.config.aws_access_key_id or os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
                                        "        secret_key = self.config.aws_secret_access_key or os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
                                        "        session_token = self.config.aws_session_token or os.getenv(\"AWS_SESSION_TOKEN\")\n",
                                        "        region = self.config.aws_region or os.getenv(\"AWS_REGION\") or \"us-east-1\"\n",
                                        "\n",
                                        "        hconf = self.spark.sparkContext._jsc.hadoopConfiguration()\n",
                                        "        hconf.set(\"fs.s3a.endpoint.region\", region)\n",
                                        "\n",
                                        "        if access_key and secret_key:\n",
                                        "            hconf.set(\"fs.s3a.access.key\", access_key)\n",
                                        "            hconf.set(\"fs.s3a.secret.key\", secret_key)\n",
                                        "            if session_token:\n",
                                        "                hconf.set(\"fs.s3a.session.token\", session_token)\n",
                                        "                hconf.set(\n",
                                        "                    \"fs.s3a.aws.credentials.provider\",\n",
                                        "                    \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\",\n",
                                        "                )\n",
                                        "            else:\n",
                                        "                hconf.set(\n",
                                        "                    \"fs.s3a.aws.credentials.provider\",\n",
                                        "                    \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
                                        "                )\n",
                                        "            boto3.Session(\n",
                                        "                aws_access_key_id=access_key,\n",
                                        "                aws_secret_access_key=secret_key,\n",
                                        "                aws_session_token=session_token,\n",
                                        "                region_name=region,\n",
                                        "            )\n",
                                        "            print(\"S3 authentication: explicit AWS credentials\")\n",
                                        "        else:\n",
                                        "            hconf.set(\n",
                                        "                \"fs.s3a.aws.credentials.provider\",\n",
                                        "                \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\",\n",
                                        "            )\n",
                                        "            boto3.Session(region_name=region)\n",
                                        "            print(\"S3 authentication: default AWS credential chain\")\n",
                                        "\n",
                                        "        self._s3_configured = True\n",
                                        "\n",
                                        "    @staticmethod\n",
                                        "    def _is_s3_path(path: str) -> bool:\n",
                                        "        return path.startswith(\"s3://\") or path.startswith(\"s3a://\")\n",
                                        "\n",
                                        "    def _iter_partition_paths(self, base_path: str) -> List[str]:\n",
                                        "        if not self.config.partition_column:\n",
                                        "            return []\n",
                                        "        if not self.config.filter_date_start or not self.config.filter_date_end:\n",
                                        "            return []\n",
                                        "\n",
                                        "        start = datetime.strptime(self.config.filter_date_start, self.config.date_format)\n",
                                        "        end = datetime.strptime(self.config.filter_date_end, self.config.date_format)\n",
                                        "\n",
                                        "        paths = []\n",
                                        "        current = start\n",
                                        "        while current <= end:\n",
                                        "            date_str = current.strftime(self.config.date_format)\n",
                                        "            paths.append(f\"{base_path}{self.config.partition_column}={date_str}/\")\n",
                                        "            current += timedelta(days=1)\n",
                                        "        return paths\n",
                                        "\n",
                                        "    def _load_from_s3(self, path: str) -> DataFrame:\n",
                                        "        self._configure_s3_authentication()\n",
                                        "        base_path = path if path.endswith(\"/\") else f\"{path}/\"\n",
                                        "\n",
                                        "        partition_paths = self._iter_partition_paths(base_path)\n",
                                        "        if partition_paths:\n",
                                        "            existing_paths = []\n",
                                        "            for p in partition_paths:\n",
                                        "                try:\n",
                                        "                    probe = self.spark.read.format(\"parquet\").load(p)\n",
                                        "                    if probe.head(1):\n",
                                        "                        existing_paths.append(p)\n",
                                        "                except Exception:\n",
                                        "                    pass\n",
                                        "\n",
                                        "            if existing_paths:\n",
                                        "                return self.spark.read.format(\"parquet\").option(\"basePath\", base_path).load(*existing_paths)\n",
                                        "\n",
                                        "        return self.spark.read.format(\"parquet\").load(base_path)\n",
                                        "\n",
                                        "    def _apply_datetime_filters(self, df: DataFrame, datetime_col: str) -> DataFrame:\n",
                                        "        if self.config.skip_date_filter:\n",
                                        "            return df\n",
                                        "        if not self.config.filter_date_start or not self.config.filter_date_end:\n",
                                        "            return df\n",
                                        "        if datetime_col not in df.columns:\n",
                                        "            return df\n",
                                        "\n",
                                        "        col_type = self.config.infer_datetime_type(datetime_col)\n",
                                        "        start_dt = datetime.strptime(self.config.filter_date_start, self.config.date_format)\n",
                                        "        end_dt = datetime.strptime(self.config.filter_date_end, self.config.date_format) + timedelta(days=1)\n",
                                        "\n",
                                        "        if col_type == \"timestamp\":\n",
                                        "            return df.filter((col(datetime_col) >= lit(start_dt)) & (col(datetime_col) < lit(end_dt)))\n",
                                        "        if col_type == \"iso\":\n",
                                        "            return df.filter(\n",
                                        "                (col(datetime_col) >= lit(start_dt.isoformat())) &\n",
                                        "                (col(datetime_col) < lit(end_dt.isoformat()))\n",
                                        "            )\n",
                                        "        return df.filter(\n",
                                        "            (col(datetime_col) >= lit(self.config.filter_date_start)) &\n",
                                        "            (col(datetime_col) <= lit(self.config.filter_date_end))\n",
                                        "        )\n",
                                        "\n",
                                        "    def _apply_date_filters(self, df: DataFrame) -> DataFrame:\n",
                                        "        if not self.config.datetime_columns:\n",
                                        "            return df\n",
                                        "        if not self.config.filter_date_start or not self.config.filter_date_end:\n",
                                        "            return df\n",
                                        "        if self.config.skip_date_filter:\n",
                                        "            return df\n",
                                        "\n",
                                        "        filtered = df\n",
                                        "        for dt_col in self.config.datetime_columns:\n",
                                        "            if dt_col in filtered.columns:\n",
                                        "                filtered = self._apply_datetime_filters(filtered, dt_col)\n",
                                        "        return filtered\n",
                                        "\n",
                                        "    def _load_source(self, table_or_path: str) -> DataFrame:\n",
                                        "        \"\"\"Load a source from S3 parquet path or Spark catalog table, then filter.\"\"\"\n",
                                        "        if self._is_s3_path(table_or_path):\n",
                                        "            df = self._load_from_s3(table_or_path)\n",
                                        "        else:\n",
                                        "            df = self.spark.table(table_or_path)\n",
                                        "\n",
                                        "        df = self._apply_date_filters(df)\n",
                                        "        return df\n",
                                        "\n",
                                        "    def load_source_1_data(self) -> DataFrame:\n",
                                        "        print(f\"Loading source_1: {self.config.source_1_table}\")\n",
                                        "        self.df_source_1 = self._load_source(self.config.source_1_table)\n",
                                        "        if(self.config.enable_persist):\n",
                                        "            self.df_source_1 = self._safe_persist(self.df_source_1)\n",
                                        "        print(f\"source_1 rows: {self.df_source_1.count():,}\")\n",
                                        "        return self.df_source_1\n",
                                        "\n",
                                        "    def load_source_2_data(self) -> DataFrame:\n",
                                        "        print(f\"Loading source_2: {self.config.source_2_table}\")\n",
                                        "        self.df_source_2 = self._load_source(self.config.source_2_table)\n",
                                        "        if(self.config.enable_persist):\n",
                                        "            self.df_source_2 = self._safe_persist(self.df_source_2)\n",
                                        "        print(f\"source_2 rows: {self.df_source_2.count():,}\")\n",
                                        "        return self.df_source_2\n",
                                        "\n",
                                        "    def _require_loaded_data(self) -> Tuple[DataFrame, DataFrame]:\n",
                                        "        if self.df_source_1 is None or self.df_source_2 is None:\n",
                                        "            raise ValueError(\"Source data is not loaded. Call load_source_1_data() and load_source_2_data() first.\")\n",
                                        "        return self.df_source_1, self.df_source_2\n",
                                        "\n",
                                        "    def compare_counts(self) -> Dict[str, int]:\n",
                                        "        df_source_1, df_source_2 = self._require_loaded_data()\n",
                                        "        source_1_count = df_source_1.count()\n",
                                        "        source_2_count = df_source_2.count()\n",
                                        "        return {\n",
                                        "            \"source_1_count\": source_1_count,\n",
                                        "            \"source_2_count\": source_2_count,\n",
                                        "            \"count_difference\": source_1_count - source_2_count,\n",
                                        "        }\n",
                                        "\n",
                                        "    def analyze_columns(self, df_source_1: DataFrame, df_source_2: DataFrame) -> pd.DataFrame:\n",
                                        "        common_cols = sorted(set(df_source_1.columns).intersection(set(df_source_2.columns)))\n",
                                        "\n",
                                        "        records = []\n",
                                        "        for column_name in common_cols:\n",
                                        "            stats_1 = df_source_1.select(\n",
                                        "                count(lit(1)).alias(\"total_count\"),\n",
                                        "                count(when(col(column_name).isNull(), 1)).alias(\"null_count\"),\n",
                                        "                countDistinct(col(column_name)).alias(\"distinct_count\"),\n",
                                        "            ).collect()[0]\n",
                                        "\n",
                                        "            stats_2 = df_source_2.select(\n",
                                        "                count(lit(1)).alias(\"total_count\"),\n",
                                        "                count(when(col(column_name).isNull(), 1)).alias(\"null_count\"),\n",
                                        "                countDistinct(col(column_name)).alias(\"distinct_count\"),\n",
                                        "            ).collect()[0]\n",
                                        "\n",
                                        "            total_1 = max(int(stats_1[\"total_count\"]), 1)\n",
                                        "            total_2 = max(int(stats_2[\"total_count\"]), 1)\n",
                                        "\n",
                                        "            records.append({\n",
                                        "                \"column_name\": column_name,\n",
                                        "                \"null_count_source_1\": int(stats_1[\"null_count\"]),\n",
                                        "                \"null_count_source_2\": int(stats_2[\"null_count\"]),\n",
                                        "                \"distinct_count_source_1\": int(stats_1[\"distinct_count\"]),\n",
                                        "                \"distinct_count_source_2\": int(stats_2[\"distinct_count\"]),\n",
                                        "                \"total_count_source_1\": int(stats_1[\"total_count\"]),\n",
                                        "                \"total_count_source_2\": int(stats_2[\"total_count\"]),\n",
                                        "                \"null_pct_source_1\": round((int(stats_1[\"null_count\"]) / total_1) * 100, 2),\n",
                                        "                \"null_pct_source_2\": round((int(stats_2[\"null_count\"]) / total_2) * 100, 2),\n",
                                        "            })\n",
                                        "\n",
                                        "        return pd.DataFrame(records)\n",
                                        "\n",
                                        "    def find_missing_records(self, key_column: str):\n",
                                        "        \"\"\"Return anti-join results for records missing in each source by key.\"\"\"\n",
                                        "        df_source_1, df_source_2 = self._require_loaded_data()\n",
                                        "\n",
                                        "        if key_column not in df_source_1.columns or key_column not in df_source_2.columns:\n",
                                        "            raise ValueError(f\"Key column '{key_column}' must exist in both sources\")\n",
                                        "\n",
                                        "        missing_in_source_2 = df_source_1.join(\n",
                                        "            df_source_2.select(key_column).distinct(),\n",
                                        "            on=[key_column],\n",
                                        "            how=\"left_anti\",\n",
                                        "        )\n",
                                        "\n",
                                        "        missing_in_source_1 = df_source_2.join(\n",
                                        "            df_source_1.select(key_column).distinct(),\n",
                                        "            on=[key_column],\n",
                                        "            how=\"left_anti\",\n",
                                        "        )\n",
                                        "\n",
                                        "        return missing_in_source_2, missing_in_source_1\n",
                                        "\n",
                                        "    def _convert_datetime_to_date(self, df: DataFrame, datetime_col: str) -> DataFrame:\n",
                                        "        if datetime_col not in df.columns:\n",
                                        "            return df\n",
                                        "\n",
                                        "        datetime_type = self.config.infer_datetime_type(datetime_col)\n",
                                        "        date_col = self.config.get_standardized_date_column_name(datetime_col)\n",
                                        "\n",
                                        "        if datetime_type == \"yyyymmdd\":\n",
                                        "            return df.withColumn(date_col, to_date(col(datetime_col).cast(\"string\"), \"yyyyMMdd\"))\n",
                                        "        if datetime_type == \"iso\":\n",
                                        "            return df.withColumn(date_col, to_date(col(datetime_col)))\n",
                                        "        return df.withColumn(date_col, to_date(col(datetime_col).cast(\"timestamp\")))\n",
                                        "\n",
                                        "    def _filter_by_date_col(self, df: DataFrame, date_col: str) -> DataFrame:\n",
                                        "        if self.config.skip_date_filter:\n",
                                        "            return df\n",
                                        "        if not self.config.filter_date_start or not self.config.filter_date_end:\n",
                                        "            return df\n",
                                        "        if date_col not in df.columns:\n",
                                        "            return df\n",
                                        "\n",
                                        "        start_date = datetime.strptime(self.config.filter_date_start, self.config.date_format).date()\n",
                                        "        end_date = datetime.strptime(self.config.filter_date_end, self.config.date_format).date()\n",
                                        "\n",
                                        "        filtered = df.filter(\n",
                                        "            (col(date_col).isNotNull()) &\n",
                                        "            (col(date_col) >= lit(start_date)) &\n",
                                        "            (col(date_col) <= lit(end_date))\n",
                                        "        )\n",
                                        "        if filtered.count() == 0 and df.count() > 0:\n",
                                        "            print(f\"Warning: no valid dates found in {date_col}; using unfiltered data.\")\n",
                                        "            return df\n",
                                        "        return filtered\n",
                                        "\n",
                                        "    def analyze_by_date(\n",
                                        "        self, df_source_1: DataFrame, df_source_2: DataFrame, key_column: str, datetime_col: Optional[str] = None\n",
                                        "    ) -> pd.DataFrame:\n",
                                        "        if datetime_col is None:\n",
                                        "            if not self.config.datetime_columns:\n",
                                        "                return pd.DataFrame(columns=[\"date\", \"source_1_count\", \"source_2_count\", \"difference\"])\n",
                                        "            datetime_col = self.config.datetime_columns[0]\n",
                                        "\n",
                                        "        df_source_1 = self._convert_datetime_to_date(df_source_1, datetime_col)\n",
                                        "        df_source_2 = self._convert_datetime_to_date(df_source_2, datetime_col)\n",
                                        "        date_col = self.config.get_standardized_date_column_name(datetime_col)\n",
                                        "\n",
                                        "        if date_col not in df_source_1.columns or date_col not in df_source_2.columns:\n",
                                        "            return pd.DataFrame(columns=[\"date\", \"source_1_count\", \"source_2_count\", \"difference\"])\n",
                                        "\n",
                                        "        df_source_1 = self._filter_by_date_col(df_source_1, date_col)\n",
                                        "        df_source_2 = self._filter_by_date_col(df_source_2, date_col)\n",
                                        "\n",
                                        "        source_1_by_date = df_source_1.groupBy(date_col).agg(count(key_column).alias(\"source_1_count\"))\n",
                                        "        source_2_by_date = df_source_2.groupBy(date_col).agg(count(key_column).alias(\"source_2_count\"))\n",
                                        "\n",
                                        "        joined = source_1_by_date.join(source_2_by_date, on=[date_col], how=\"outer\").fillna(0)\n",
                                        "        joined = joined.withColumn(\"difference\", col(\"source_1_count\") - col(\"source_2_count\"))\n",
                                        "        joined = joined.withColumn(date_col, col(date_col).cast(\"string\"))\n",
                                        "        return joined.orderBy(date_col).toPandas().rename(columns={date_col: \"date\"})\n",
                                        "\n",
                                        "    def recheck_missing_records(self, key_column: Optional[str] = None) -> Dict:\n",
                                        "        key_column = key_column or self.config.primary_key\n",
                                        "        df_source_1, df_source_2 = self._require_loaded_data()\n",
                                        "\n",
                                        "        if \"missing_in_source_2\" not in self.results or \"missing_in_source_1\" not in self.results:\n",
                                        "            missing_in_source_2, missing_in_source_1 = self.find_missing_records(key_column)\n",
                                        "        else:\n",
                                        "            missing_in_source_2 = self.results[\"missing_in_source_2\"]\n",
                                        "            missing_in_source_1 = self.results[\"missing_in_source_1\"]\n",
                                        "\n",
                                        "        source_2_keys = df_source_2.select(key_column).distinct()\n",
                                        "        source_1_keys = df_source_1.select(key_column).distinct()\n",
                                        "\n",
                                        "        still_missing_in_source_2 = missing_in_source_2.join(source_2_keys, on=[key_column], how=\"left_anti\")\n",
                                        "        still_missing_in_source_1 = missing_in_source_1.join(source_1_keys, on=[key_column], how=\"left_anti\")\n",
                                        "\n",
                                        "        result = {\n",
                                        "            \"missing_in_source_2_original\": missing_in_source_2.count(),\n",
                                        "            \"missing_in_source_1_original\": missing_in_source_1.count(),\n",
                                        "            \"still_missing_in_source_2\": still_missing_in_source_2.count(),\n",
                                        "            \"still_missing_in_source_1\": still_missing_in_source_1.count(),\n",
                                        "            \"found_in_source_2_on_recheck\": missing_in_source_2.count() - still_missing_in_source_2.count(),\n",
                                        "            \"found_in_source_1_on_recheck\": missing_in_source_1.count() - still_missing_in_source_1.count(),\n",
                                        "            \"still_missing_source_2_df\": still_missing_in_source_2,\n",
                                        "            \"still_missing_source_1_df\": still_missing_in_source_1,\n",
                                        "        }\n",
                                        "        self.results[\"recheck_results\"] = result\n",
                                        "        return result\n",
                                        "\n",
                                        "    def print_recheck_results(self):\n",
                                        "        if \"recheck_results\" not in self.results:\n",
                                        "            print(\"No recheck results found. Run recheck_missing_records() first.\")\n",
                                        "            return\n",
                                        "\n",
                                        "        r = self.results[\"recheck_results\"]\n",
                                        "        print(\"=\" * 80)\n",
                                        "        print(\"Recheck Summary\")\n",
                                        "        print(\"=\" * 80)\n",
                                        "        print(f\"Original missing in source_2: {r['missing_in_source_2_original']:,}\")\n",
                                        "        print(f\"Found in source_2 on recheck: {r['found_in_source_2_on_recheck']:,}\")\n",
                                        "        print(f\"Still missing in source_2: {r['still_missing_in_source_2']:,}\")\n",
                                        "        print(f\"Original missing in source_1: {r['missing_in_source_1_original']:,}\")\n",
                                        "        print(f\"Found in source_1 on recheck: {r['found_in_source_1_on_recheck']:,}\")\n",
                                        "        print(f\"Still missing in source_1: {r['still_missing_in_source_1']:,}\")\n",
                                        "\n",
                                        "    def _analyze_missing_by_date_impl(self, missing_df: DataFrame, source_label: str, datetime_col: str, key_column: str) -> pd.DataFrame:\n",
                                        "        if missing_df is None:\n",
                                        "            return pd.DataFrame(columns=[\"date\", \"missing_count\", \"source\"])\n",
                                        "\n",
                                        "        df = self._convert_datetime_to_date(missing_df, datetime_col)\n",
                                        "        date_col = self.config.get_standardized_date_column_name(datetime_col)\n",
                                        "        if date_col not in df.columns:\n",
                                        "            return pd.DataFrame(columns=[\"date\", \"missing_count\", \"source\"])\n",
                                        "\n",
                                        "        df = self._filter_by_date_col(df, date_col)\n",
                                        "        out = df.groupBy(date_col).agg(count(key_column).alias(\"missing_count\")).orderBy(date_col)\n",
                                        "        out = out.withColumn(date_col, col(date_col).cast(\"string\"))\n",
                                        "        out = out.toPandas().rename(columns={date_col: \"date\"})\n",
                                        "        out[\"source\"] = source_label\n",
                                        "        return out\n",
                                        "\n",
                                        "    def analyze_missing_by_date(\n",
                                        "        self, key_column: Optional[str] = None, datetime_col: Optional[str] = None\n",
                                        "    ) -> Dict[str, pd.DataFrame]:\n",
                                        "        key_column = key_column or self.config.primary_key\n",
                                        "        if datetime_col is None:\n",
                                        "            if not self.config.datetime_columns:\n",
                                        "                empty = pd.DataFrame(columns=[\"date\", \"missing_count\", \"source\"])\n",
                                        "                return {\"missing_in_source_2_by_date\": empty, \"missing_in_source_1_by_date\": empty}\n",
                                        "            datetime_col = self.config.datetime_columns[0]\n",
                                        "\n",
                                        "        if \"missing_in_source_2\" not in self.results or \"missing_in_source_1\" not in self.results:\n",
                                        "            self.results[\"missing_in_source_2\"], self.results[\"missing_in_source_1\"] = self.find_missing_records(key_column)\n",
                                        "\n",
                                        "        source_2_pdf = self._analyze_missing_by_date_impl(\n",
                                        "            self.results[\"missing_in_source_2\"], \"source_2\", datetime_col, key_column\n",
                                        "        )\n",
                                        "        source_1_pdf = self._analyze_missing_by_date_impl(\n",
                                        "            self.results[\"missing_in_source_1\"], \"source_1\", datetime_col, key_column\n",
                                        "        )\n",
                                        "\n",
                                        "        self.results[\"missing_in_source_2_by_date\"] = source_2_pdf\n",
                                        "        self.results[\"missing_in_source_1_by_date\"] = source_1_pdf\n",
                                        "        return {\n",
                                        "            \"missing_in_source_2_by_date\": source_2_pdf,\n",
                                        "            \"missing_in_source_1_by_date\": source_1_pdf,\n",
                                        "        }\n",
                                        "\n",
                                        "    def analyze_nulls_by_date(\n",
                                        "        self, df: DataFrame, key_column: str, datetime_col: str, source_label: Optional[str] = None\n",
                                        "    ) -> pd.DataFrame:\n",
                                        "        df = self._convert_datetime_to_date(df, datetime_col)\n",
                                        "        date_col = self.config.get_standardized_date_column_name(datetime_col)\n",
                                        "        if date_col not in df.columns:\n",
                                        "            return pd.DataFrame(columns=[\"date\", \"total_count\", \"key_null_count\", \"key_null_pct\", \"source\"])\n",
                                        "\n",
                                        "        df = self._filter_by_date_col(df, date_col)\n",
                                        "        out = df.groupBy(date_col).agg(\n",
                                        "            count(lit(1)).alias(\"total_count\"),\n",
                                        "            count(when(col(key_column).isNull(), 1)).alias(\"key_null_count\"),\n",
                                        "        )\n",
                                        "        out = out.withColumn(\"key_null_pct\", (col(\"key_null_count\") * lit(100.0) / col(\"total_count\")))\n",
                                        "        out = out.withColumn(date_col, col(date_col).cast(\"string\"))\n",
                                        "        pdf = out.orderBy(date_col).toPandas().rename(columns={date_col: \"date\"})\n",
                                        "        if source_label:\n",
                                        "            pdf[\"source\"] = source_label\n",
                                        "        return pdf\n",
                                        "\n",
                                        "    def analyze_columns_with_datatypes(self, df: DataFrame, source_name: str) -> pd.DataFrame:\n",
                                        "        rows = []\n",
                                        "        total_count = df.count()\n",
                                        "        for field in df.schema.fields:\n",
                                        "            column_name = field.name\n",
                                        "            null_count = df.select(count(when(col(column_name).isNull(), 1)).alias(\"n\")).collect()[0][\"n\"]\n",
                                        "            distinct_count = df.select(countDistinct(col(column_name)).alias(\"d\")).collect()[0][\"d\"]\n",
                                        "            rows.append({\n",
                                        "                \"source\": source_name,\n",
                                        "                \"column_name\": column_name,\n",
                                        "                \"data_type\": str(field.dataType),\n",
                                        "                \"null_count\": int(null_count),\n",
                                        "                \"distinct_count\": int(distinct_count),\n",
                                        "                \"total_count\": int(total_count),\n",
                                        "                \"null_pct\": round((int(null_count) / max(total_count, 1)) * 100, 2),\n",
                                        "            })\n",
                                        "        return pd.DataFrame(rows)\n",
                                        "\n",
                                        "    def compare_column_schemas(self) -> Dict[str, List[str]]:\n",
                                        "        df_source_1, df_source_2 = self._require_loaded_data()\n",
                                        "        source_1_cols = set(df_source_1.columns)\n",
                                        "        source_2_cols = set(df_source_2.columns)\n",
                                        "        return {\n",
                                        "            \"only_in_source_1\": sorted(source_1_cols - source_2_cols),\n",
                                        "            \"only_in_source_2\": sorted(source_2_cols - source_1_cols),\n",
                                        "            \"common\": sorted(source_1_cols & source_2_cols),\n",
                                        "        }\n",
                                        "\n",
                                        "    def highlight_high_null_columns(self, threshold_pct: float = 80.0) -> Dict[str, pd.DataFrame]:\n",
                                        "        if \"column_analysis\" not in self.results:\n",
                                        "            df_source_1, df_source_2 = self._require_loaded_data()\n",
                                        "            self.results[\"column_analysis\"] = self.analyze_columns(df_source_1, df_source_2)\n",
                                        "\n",
                                        "        column_analysis = self.results[\"column_analysis\"]\n",
                                        "        source_1_high = column_analysis[column_analysis[\"null_pct_source_1\"] >= threshold_pct][[\n",
                                        "            \"column_name\", \"null_pct_source_1\", \"null_count_source_1\", \"total_count_source_1\"\n",
                                        "        ]]\n",
                                        "        source_2_high = column_analysis[column_analysis[\"null_pct_source_2\"] >= threshold_pct][[\n",
                                        "            \"column_name\", \"null_pct_source_2\", \"null_count_source_2\", \"total_count_source_2\"\n",
                                        "        ]]\n",
                                        "\n",
                                        "        result = {\n",
                                        "            \"source_1_high_nulls\": source_1_high,\n",
                                        "            \"source_2_high_nulls\": source_2_high,\n",
                                        "        }\n",
                                        "        self.results[\"high_null_analysis\"] = result\n",
                                        "        return result\n",
                                        "\n",
                                        "    def run_full_analysis(self) -> Dict:\n",
                                        "        \"\"\"Execute baseline comparison workflow and store all generated artifacts.\"\"\"\n",
                                        "        self.load_source_1_data()\n",
                                        "        self.load_source_2_data()\n",
                                        "\n",
                                        "        # Baseline count and column-level health metrics.\n",
                                        "        self.results.update(self.compare_counts())\n",
                                        "        df_source_1, df_source_2 = self._require_loaded_data()\n",
                                        "        self.results[\"column_analysis\"] = self.analyze_columns(df_source_1, df_source_2)\n",
                                        "\n",
                                        "        # Date-level record volume comparison for each configured datetime column.\n",
                                        "        for datetime_col in self.config.datetime_columns:\n",
                                        "            date_analysis = self.analyze_by_date(\n",
                                        "                df_source_1, df_source_2, self.config.count_key, datetime_col\n",
                                        "            )\n",
                                        "            self.results[f\"date_analysis_{datetime_col}\"] = date_analysis\n",
                                        "        if self.config.datetime_columns:\n",
                                        "            first_datetime = self.config.datetime_columns[0]\n",
                                        "            self.results[\"date_analysis\"] = self.results.get(f\"date_analysis_{first_datetime}\")\n",
                                        "\n",
                                        "        # Key-level anti-join checks to find records absent in each source.\n",
                                        "        missing_in_source_2, missing_in_source_1 = self.find_missing_records(self.config.primary_key)\n",
                                        "        self.results[\"missing_in_source_2\"] = missing_in_source_2\n",
                                        "        self.results[\"missing_in_source_1\"] = missing_in_source_1\n",
                                        "\n",
                                        "        for datetime_col in self.config.datetime_columns:\n",
                                        "            missing_by_date = self.analyze_missing_by_date(self.config.primary_key, datetime_col)\n",
                                        "            self.results[f\"missing_by_date_{datetime_col}\"] = missing_by_date\n",
                                        "        if self.config.datetime_columns:\n",
                                        "            first_datetime = self.config.datetime_columns[0]\n",
                                        "            self.results[\"missing_by_date\"] = self.results.get(f\"missing_by_date_{first_datetime}\")\n",
                                        "\n",
                                        "        for datetime_col in self.config.datetime_columns:\n",
                                        "            nulls_source_1 = self.analyze_nulls_by_date(\n",
                                        "                df_source_1, self.config.primary_key, datetime_col, \"source_1\"\n",
                                        "            )\n",
                                        "            nulls_source_2 = self.analyze_nulls_by_date(\n",
                                        "                df_source_2, self.config.primary_key, datetime_col, \"source_2\"\n",
                                        "            )\n",
                                        "            self.results[f\"nulls_by_date_{datetime_col}\"] = {\n",
                                        "                \"source_1\": nulls_source_1,\n",
                                        "                \"source_2\": nulls_source_2,\n",
                                        "            }\n",
                                        "\n",
                                        "        return self.results\n",
                                        "\n",
                                        "    def display_summary(self):\n",
                                        "        if not self.results:\n",
                                        "            print(\"No results found. Run full analysis first.\")\n",
                                        "            return\n",
                                        "\n",
                                        "        print(\"=\" * 80)\n",
                                        "        print(f\"Summary: {self.config.domain_name}\")\n",
                                        "        print(\"=\" * 80)\n",
                                        "        print(f\"source_1 table: {self.config.source_1_table}\")\n",
                                        "        print(f\"source_2 table: {self.config.source_2_table}\")\n",
                                        "        print(f\"source_1 count: {self.results['source_1_count']:,}\")\n",
                                        "        print(f\"source_2 count: {self.results['source_2_count']:,}\")\n",
                                        "        print(f\"difference: {self.results['count_difference']:,}\")\n",
                                        "        print(f\"missing in source_2: {self.results['missing_in_source_2'].count():,}\")\n",
                                        "        print(f\"missing in source_1: {self.results['missing_in_source_1'].count():,}\")\n",
                                        "\n",
                                        "    def _cast_datetime_columns_to_string(self, df: DataFrame) -> DataFrame:\n",
                                        "        for field in df.schema.fields:\n",
                                        "            dtype = str(field.dataType).lower()\n",
                                        "            if \"date\" in dtype or \"timestamp\" in dtype:\n",
                                        "                df = df.withColumn(field.name, col(field.name).cast(\"string\"))\n",
                                        "        return df\n",
                                        "\n",
                                        "    def export_results(self):\n",
                                        "        if \"column_analysis\" not in self.results:\n",
                                        "            raise ValueError(\"No analysis results available. Run full analysis first.\")\n",
                                        "\n",
                                        "        self.results[\"column_analysis\"].to_excel(self.config.excel_path, index=False)\n",
                                        "        self.results[\"column_analysis\"].to_csv(self.config.csv_path, index=False)\n",
                                        "\n",
                                        "        missing_source_2_path = os.path.join(\n",
                                        "            self.config.output_base_path,\n",
                                        "            f\"{self.config.domain_name}_missing_in_source_2.xlsx\",\n",
                                        "        )\n",
                                        "        missing_source_1_path = os.path.join(\n",
                                        "            self.config.output_base_path,\n",
                                        "            f\"{self.config.domain_name}_missing_in_source_1.xlsx\",\n",
                                        "        )\n",
                                        "\n",
                                        "        missing_source_2 = self._cast_datetime_columns_to_string(self.results[\"missing_in_source_2\"])\n",
                                        "        missing_source_1 = self._cast_datetime_columns_to_string(self.results[\"missing_in_source_1\"])\n",
                                        "        missing_source_2.toPandas().to_excel(missing_source_2_path, index=False)\n",
                                        "        missing_source_1.toPandas().to_excel(missing_source_1_path, index=False)\n",
                                        "\n",
                                        "        print(f\"Saved: {self.config.excel_path}\")\n",
                                        "        print(f\"Saved: {self.config.csv_path}\")\n",
                                        "        print(f\"Saved: {missing_source_2_path}\")\n",
                                        "        print(f\"Saved: {missing_source_1_path}\")\n",
                                        "\n",
                                        "    def run_enhanced_analysis(self) -> Dict:\n",
                                        "        df_source_1, df_source_2 = self._require_loaded_data()\n",
                                        "\n",
                                        "        source_1_enhanced = self.analyze_columns_with_datatypes(df_source_1, \"source_1\")\n",
                                        "        source_2_enhanced = self.analyze_columns_with_datatypes(df_source_2, \"source_2\")\n",
                                        "        schema_comparison = self.compare_column_schemas()\n",
                                        "        high_null_analysis = self.highlight_high_null_columns()\n",
                                        "\n",
                                        "        enhanced = {\n",
                                        "            \"source_1_enhanced_analysis\": source_1_enhanced,\n",
                                        "            \"source_2_enhanced_analysis\": source_2_enhanced,\n",
                                        "            \"schema_comparison\": schema_comparison,\n",
                                        "            \"high_null_analysis\": high_null_analysis,\n",
                                        "        }\n",
                                        "        self.results.update(enhanced)\n",
                                        "        return enhanced\n",
                                        "\n",
                                        "    def export_enhanced_to_excel(self, output_path: Optional[str] = None):\n",
                                        "        if \"schema_comparison\" not in self.results or \"high_null_analysis\" not in self.results:\n",
                                        "            raise ValueError(\"Enhanced analysis not found. Run run_enhanced_analysis() first.\")\n",
                                        "\n",
                                        "        output_path = output_path or os.path.join(\n",
                                        "            self.config.output_base_path, f\"{self.config.domain_name}_enhanced.xlsx\"\n",
                                        "        )\n",
                                        "        with pd.ExcelWriter(output_path, engine=\"openpyxl\") as writer:\n",
                                        "            self.results[\"source_1_enhanced_analysis\"].to_excel(\n",
                                        "                writer, sheet_name=\"Source1Columns\", index=False\n",
                                        "            )\n",
                                        "            self.results[\"source_2_enhanced_analysis\"].to_excel(\n",
                                        "                writer, sheet_name=\"Source2Columns\", index=False\n",
                                        "            )\n",
                                        "            pd.DataFrame(self.results[\"schema_comparison\"][\"only_in_source_1\"], columns=[\"only_in_source_1\"]).to_excel(\n",
                                        "                writer, sheet_name=\"OnlyInSource1\", index=False\n",
                                        "            )\n",
                                        "            pd.DataFrame(self.results[\"schema_comparison\"][\"only_in_source_2\"], columns=[\"only_in_source_2\"]).to_excel(\n",
                                        "                writer, sheet_name=\"OnlyInSource2\", index=False\n",
                                        "            )\n",
                                        "            pd.DataFrame(self.results[\"schema_comparison\"][\"common\"], columns=[\"common\"]).to_excel(\n",
                                        "                writer, sheet_name=\"CommonColumns\", index=False\n",
                                        "            )\n",
                                        "            self.results[\"high_null_analysis\"][\"source_1_high_nulls\"].to_excel(\n",
                                        "                writer, sheet_name=\"HighNullsSource1\", index=False\n",
                                        "            )\n",
                                        "            self.results[\"high_null_analysis\"][\"source_2_high_nulls\"].to_excel(\n",
                                        "                writer, sheet_name=\"HighNullsSource2\", index=False\n",
                                        "            )\n",
                                        "\n",
                                        "        print(f\"Saved: {output_path}\")\n",
                                        "\n",
                                        "    def export_to_excel(self):\n",
                                        "        \"\"\"Export baseline and date-based outputs into a multi-sheet Excel workbook.\"\"\"\n",
                                        "        if \"column_analysis\" not in self.results:\n",
                                        "            raise ValueError(\"No analysis results available. Run full analysis first.\")\n",
                                        "\n",
                                        "        with pd.ExcelWriter(self.config.excel_path, engine=\"openpyxl\") as writer:\n",
                                        "            self.results[\"column_analysis\"].to_excel(writer, sheet_name=\"ColumnAnalysis\", index=False)\n",
                                        "\n",
                                        "            for datetime_col in self.config.datetime_columns:\n",
                                        "                date_key = f\"date_analysis_{datetime_col}\"\n",
                                        "                if date_key in self.results and not self.results[date_key].empty:\n",
                                        "                    sheet = f\"Dates_{datetime_col}\"[:31]\n",
                                        "                    self.results[date_key].to_excel(writer, sheet_name=sheet, index=False)\n",
                                        "\n",
                                        "                missing_key = f\"missing_by_date_{datetime_col}\"\n",
                                        "                if missing_key in self.results:\n",
                                        "                    missing_by_date = self.results[missing_key]\n",
                                        "                    if not missing_by_date[\"missing_in_source_2_by_date\"].empty:\n",
                                        "                        sheet = f\"MissingSrc2_{datetime_col}\"[:31]\n",
                                        "                        missing_by_date[\"missing_in_source_2_by_date\"].to_excel(writer, sheet_name=sheet, index=False)\n",
                                        "                    if not missing_by_date[\"missing_in_source_1_by_date\"].empty:\n",
                                        "                        sheet = f\"MissingSrc1_{datetime_col}\"[:31]\n",
                                        "                        missing_by_date[\"missing_in_source_1_by_date\"].to_excel(writer, sheet_name=sheet, index=False)\n",
                                        "\n",
                                        "                nulls_key = f\"nulls_by_date_{datetime_col}\"\n",
                                        "                if nulls_key in self.results:\n",
                                        "                    nulls_by_date = self.results[nulls_key]\n",
                                        "                    if not nulls_by_date[\"source_1\"].empty:\n",
                                        "                        sheet = f\"NullsSrc1_{datetime_col}\"[:31]\n",
                                        "                        nulls_by_date[\"source_1\"].to_excel(writer, sheet_name=sheet, index=False)\n",
                                        "                    if not nulls_by_date[\"source_2\"].empty:\n",
                                        "                        sheet = f\"NullsSrc2_{datetime_col}\"[:31]\n",
                                        "                        nulls_by_date[\"source_2\"].to_excel(writer, sheet_name=sheet, index=False)\n",
                                        "\n",
                                        "        print(f\"Saved: {self.config.excel_path}\")\n",
                                        "\n",
                                        "    def export_to_csv(self):\n",
                                        "        if \"column_analysis\" not in self.results:\n",
                                        "            raise ValueError(\"No analysis results available. Run full analysis first.\")\n",
                                        "        self.results[\"column_analysis\"].to_csv(self.config.csv_path, index=False)\n",
                                        "        print(f\"Saved: {self.config.csv_path}\")\n",
                                        "\n",
                                        "    def export_missing_records(self):\n",
                                        "        if \"missing_in_source_2\" not in self.results or \"missing_in_source_1\" not in self.results:\n",
                                        "            raise ValueError(\"Missing-record results not found. Run full analysis first.\")\n",
                                        "\n",
                                        "        missing_source_2_path = os.path.join(\n",
                                        "            self.config.output_base_path, f\"{self.config.domain_name}_missing_in_source_2.xlsx\"\n",
                                        "        )\n",
                                        "        missing_source_1_path = os.path.join(\n",
                                        "            self.config.output_base_path, f\"{self.config.domain_name}_missing_in_source_1.xlsx\"\n",
                                        "        )\n",
                                        "\n",
                                        "        missing_source_2 = self._cast_datetime_columns_to_string(self.results[\"missing_in_source_2\"])\n",
                                        "        missing_source_1 = self._cast_datetime_columns_to_string(self.results[\"missing_in_source_1\"])\n",
                                        "        missing_source_2.toPandas().to_excel(missing_source_2_path, index=False)\n",
                                        "        missing_source_1.toPandas().to_excel(missing_source_1_path, index=False)\n",
                                        "\n",
                                        "        print(f\"Saved: {missing_source_2_path}\")\n",
                                        "        print(f\"Saved: {missing_source_1_path}\")\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "f265cb39",
                              "metadata": {},
                              "source": [
                                        "## Batch Execution\n",
                                        "\n",
                                        "Use `run_batch_comparison(...)` to process multiple `DataComparisonConfig` objects in one run.\n",
                                        "\n",
                                        "- Continues to the next dataset even if one comparison fails.\n",
                                        "- Optionally runs enhanced schema/high-null analysis.\n",
                                        "- Optionally rechecks missing records after initial detection.\n",
                                        "- Writes per-domain outputs under each config `output_base_path`.\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 4,
                              "id": "9d3b759c",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "def run_batch_comparison(\n",
                                        "    table_configs: List[DataComparisonConfig],\n",
                                        "    export_results: bool = True,\n",
                                        "    run_enhanced_analysis: bool = False,\n",
                                        "    run_recheck: bool = True,\n",
                                        ") -> Dict:\n",
                                        "    \"\"\"Run comparisons for multiple configs and return per-domain results.\"\"\"\n",
                                        "    all_results = {}\n",
                                        "\n",
                                        "    print(\"#\" * 80)\n",
                                        "    print(f\"Running batch for {len(table_configs)} table pairs\")\n",
                                        "    print(\"#\" * 80)\n",
                                        "\n",
                                        "    for index, config in enumerate(table_configs, start=1):\n",
                                        "        print(\"=\" * 80)\n",
                                        "        print(f\"[{index}/{len(table_configs)}] {config.domain_name}\")\n",
                                        "        print(\"=\" * 80)\n",
                                        "\n",
                                        "        try:\n",
                                        "            analyzer = DataComparisonAnalyzer(config)\n",
                                        "            results = analyzer.run_full_analysis()\n",
                                        "\n",
                                        "            if run_recheck:\n",
                                        "                analyzer.recheck_missing_records(config.primary_key)\n",
                                        "\n",
                                        "            if run_enhanced_analysis:\n",
                                        "                enhanced_results = analyzer.run_enhanced_analysis()\n",
                                        "                results['enhanced'] = enhanced_results\n",
                                        "\n",
                                        "            analyzer.display_summary()\n",
                                        "\n",
                                        "            if export_results:\n",
                                        "                analyzer.export_to_excel()\n",
                                        "                analyzer.export_to_csv()\n",
                                        "                analyzer.export_missing_records()\n",
                                        "                if run_enhanced_analysis:\n",
                                        "                    analyzer.export_enhanced_to_excel()\n",
                                        "\n",
                                        "            all_results[config.domain_name] = {\n",
                                        "                'analyzer': analyzer,\n",
                                        "                'results': results,\n",
                                        "            }\n",
                                        "            print(f\"Completed: {config.domain_name}\")\n",
                                        "        except Exception as exc:\n",
                                        "            # Keep batch execution resilient: capture error and continue.\n",
                                        "            all_results[config.domain_name] = {'error': str(exc)}\n",
                                        "            print(f\"Failed: {config.domain_name} -> {exc}\")\n",
                                        "\n",
                                        "    return all_results\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "5321530f",
                              "metadata": {},
                              "source": [
                                        "## Example Usage\n",
                                        "\n",
                                        "Use the template below for real sources (S3 or catalog tables).\n",
                                        "\n",
                                        "`run_full_analysis()` computes metrics in memory. To save files under `output_base_path`, call the export methods.\n",
                                        "\n",
                                        "```python\n",
                                        "config = DataComparisonConfig(\n",
                                        "    source_1_table=\"s3a://your-bucket/source_1/path/\",\n",
                                        "    source_2_table=\"s3a://your-bucket/source_2/path/\",\n",
                                        "    primary_key=\"id\",\n",
                                        "    domain_name=\"example_dataset\",\n",
                                        "    count_key=\"id\",\n",
                                        "    datetime_columns=[\"your_datetime_column\"],\n",
                                        "    filter_date_start=\"20250101\",\n",
                                        "    filter_date_end=\"20250131\",\n",
                                        "    partition_column=None,\n",
                                        "    aws_region=\"us-east-1\",\n",
                                        "    aws_access_key_id=None,\n",
                                        "    aws_secret_access_key=None,\n",
                                        "    aws_session_token=None,\n",
                                        "    output_base_path=\"./outputs\",\n",
                                        ")\n",
                                        "\n",
                                        "analyzer = DataComparisonAnalyzer(config)\n",
                                        "results = analyzer.run_full_analysis()\n",
                                        "analyzer.display_summary()\n",
                                        "analyzer.export_to_csv()\n",
                                        "analyzer.export_to_excel()\n",
                                        "analyzer.export_missing_records()\n",
                                        "```\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 5,
                              "id": "aa34cfc2",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# Uncomment and adjust for a real dataset run.\n",
                                        "# config = DataComparisonConfig(\n",
                                        "#     source_1_table=\"s3a://your-bucket/source_1/path/\",\n",
                                        "#     source_2_table=\"s3a://your-bucket/source_2/path/\",\n",
                                        "#     primary_key=\"id\",\n",
                                        "#     domain_name=\"example_dataset\",\n",
                                        "#     count_key=\"id\",\n",
                                        "#     datetime_columns=[\"your_datetime_column\"],\n",
                                        "#     filter_date_start=\"20250101\",\n",
                                        "#     filter_date_end=\"20250131\",\n",
                                        "#     partition_column=None,\n",
                                        "#     aws_region=\"us-east-1\",\n",
                                        "#     aws_access_key_id=None,\n",
                                        "#     aws_secret_access_key=None,\n",
                                        "#     aws_session_token=None,\n",
                                        "#     output_base_path=\"./outputs\",\n",
                                        "# )\n",
                                        "#\n",
                                        "# analyzer = DataComparisonAnalyzer(config)\n",
                                        "# results = analyzer.run_full_analysis()\n",
                                        "# analyzer.display_summary()\n",
                                        "# analyzer.export_to_excel()\n",
                                        "# analyzer.export_to_csv()\n",
                                        "# analyzer.export_missing_records()\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 8,
                              "id": "b797659a",
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "Loading source_1: source_1_test\n",
                                                            "source_1 rows: 4\n",
                                                            "Loading source_2: source_2_test\n",
                                                            "source_2 rows: 3\n",
                                                            "================================================================================\n",
                                                            "Summary: smoke_test\n",
                                                            "================================================================================\n",
                                                            "source_1 table: source_1_test\n",
                                                            "source_2 table: source_2_test\n",
                                                            "source_1 count: 4\n",
                                                            "source_2 count: 3\n",
                                                            "difference: 1\n",
                                                            "missing in source_2: 2\n",
                                                            "missing in source_1: 1\n",
                                                            "Saved: ./outputs\\smoke_test_comparison.csv\n",
                                                            "Saved: ./outputs\\smoke_test_comparison.xlsx\n",
                                                            "Saved: ./outputs\\smoke_test_missing_in_source_2.xlsx\n",
                                                            "Saved: ./outputs\\smoke_test_missing_in_source_1.xlsx\n"
                                                  ]
                                        },
                                        {
                                                  "data": {
                                                            "text/html": [
                                                                      "<div>\n",
                                                                      "<style scoped>\n",
                                                                      "    .dataframe tbody tr th:only-of-type {\n",
                                                                      "        vertical-align: middle;\n",
                                                                      "    }\n",
                                                                      "\n",
                                                                      "    .dataframe tbody tr th {\n",
                                                                      "        vertical-align: top;\n",
                                                                      "    }\n",
                                                                      "\n",
                                                                      "    .dataframe thead th {\n",
                                                                      "        text-align: right;\n",
                                                                      "    }\n",
                                                                      "</style>\n",
                                                                      "<table border=\"1\" class=\"dataframe\">\n",
                                                                      "  <thead>\n",
                                                                      "    <tr style=\"text-align: right;\">\n",
                                                                      "      <th></th>\n",
                                                                      "      <th>column_name</th>\n",
                                                                      "      <th>null_count_source_1</th>\n",
                                                                      "      <th>null_count_source_2</th>\n",
                                                                      "      <th>distinct_count_source_1</th>\n",
                                                                      "      <th>distinct_count_source_2</th>\n",
                                                                      "      <th>total_count_source_1</th>\n",
                                                                      "      <th>total_count_source_2</th>\n",
                                                                      "      <th>null_pct_source_1</th>\n",
                                                                      "      <th>null_pct_source_2</th>\n",
                                                                      "    </tr>\n",
                                                                      "  </thead>\n",
                                                                      "  <tbody>\n",
                                                                      "    <tr>\n",
                                                                      "      <th>0</th>\n",
                                                                      "      <td>event_ts</td>\n",
                                                                      "      <td>0</td>\n",
                                                                      "      <td>0</td>\n",
                                                                      "      <td>4</td>\n",
                                                                      "      <td>3</td>\n",
                                                                      "      <td>4</td>\n",
                                                                      "      <td>3</td>\n",
                                                                      "      <td>0.0</td>\n",
                                                                      "      <td>0.00</td>\n",
                                                                      "    </tr>\n",
                                                                      "    <tr>\n",
                                                                      "      <th>1</th>\n",
                                                                      "      <td>id</td>\n",
                                                                      "      <td>0</td>\n",
                                                                      "      <td>0</td>\n",
                                                                      "      <td>4</td>\n",
                                                                      "      <td>3</td>\n",
                                                                      "      <td>4</td>\n",
                                                                      "      <td>3</td>\n",
                                                                      "      <td>0.0</td>\n",
                                                                      "      <td>0.00</td>\n",
                                                                      "    </tr>\n",
                                                                      "    <tr>\n",
                                                                      "      <th>2</th>\n",
                                                                      "      <td>val</td>\n",
                                                                      "      <td>1</td>\n",
                                                                      "      <td>1</td>\n",
                                                                      "      <td>3</td>\n",
                                                                      "      <td>2</td>\n",
                                                                      "      <td>4</td>\n",
                                                                      "      <td>3</td>\n",
                                                                      "      <td>25.0</td>\n",
                                                                      "      <td>33.33</td>\n",
                                                                      "    </tr>\n",
                                                                      "  </tbody>\n",
                                                                      "</table>\n",
                                                                      "</div>"
                                                            ],
                                                            "text/plain": [
                                                                      "  column_name  null_count_source_1  null_count_source_2  \\\n",
                                                                      "0    event_ts                    0                    0   \n",
                                                                      "1          id                    0                    0   \n",
                                                                      "2         val                    1                    1   \n",
                                                                      "\n",
                                                                      "   distinct_count_source_1  distinct_count_source_2  total_count_source_1  \\\n",
                                                                      "0                        4                        3                     4   \n",
                                                                      "1                        4                        3                     4   \n",
                                                                      "2                        3                        2                     4   \n",
                                                                      "\n",
                                                                      "   total_count_source_2  null_pct_source_1  null_pct_source_2  \n",
                                                                      "0                     3                0.0               0.00  \n",
                                                                      "1                     3                0.0               0.00  \n",
                                                                      "2                     3               25.0              33.33  "
                                                            ]
                                                  },
                                                  "execution_count": 8,
                                                  "metadata": {},
                                                  "output_type": "execute_result"
                                        }
                              ],
                              "source": [
                                        "# Local smoke test with in-memory sample data.\n",
                                        "# This verifies the analyzer workflow without external dependencies.\n",
                                        "spark = (\n",
                                        "    SparkSession.builder\n",
                                        "    .master(\"local[*]\")\n",
                                        "    .appName(\"TestApp\")\n",
                                        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
                                        "    .config(\"spark.connect.enabled\", \"false\")  # Avoid Spark Connect behavior in local mode\n",
                                        "    .getOrCreate()\n",
                                        ")\n",
                                        "\n",
                                        "source_1_df = spark.createDataFrame([\n",
                                        "    (1, \"2025-01-10\", \"A\"),\n",
                                        "    (2, \"2025-01-11\", \"B\"),\n",
                                        "    (3, \"2025-01-12\", None),\n",
                                        "    (4, \"2025-01-13\", \"D\"),\n",
                                        "], [\"id\", \"event_ts\", \"val\"])\n",
                                        "\n",
                                        "source_2_df = spark.createDataFrame([\n",
                                        "    (1, \"2025-01-10\", \"A\"),\n",
                                        "    (2, \"2025-01-11\", None),\n",
                                        "    (5, \"2025-01-14\", \"E\"),\n",
                                        "], [\"id\", \"event_ts\", \"val\"])\n",
                                        "\n",
                                        "source_1_df.createOrReplaceTempView(\"source_1_test\")\n",
                                        "source_2_df.createOrReplaceTempView(\"source_2_test\")\n",
                                        "\n",
                                        "config = DataComparisonConfig(\n",
                                        "    source_1_table=\"source_1_test\",\n",
                                        "    source_2_table=\"source_2_test\",\n",
                                        "    primary_key=\"id\",\n",
                                        "    count_key=\"id\",\n",
                                        "    domain_name=\"smoke_test\",\n",
                                        "    datetime_columns=[\"event_ts\"],\n",
                                        "    filter_date_start=\"20250101\",\n",
                                        "    filter_date_end=\"20250131\",\n",
                                        "    output_base_path=\"./outputs\",\n",
                                        "    enable_persist=False,  # Keep disabled in local-only smoke tests\n",
                                        ")\n",
                                        "\n",
                                        "analyzer = DataComparisonAnalyzer(config)\n",
                                        "results = analyzer.run_full_analysis()\n",
                                        "analyzer.display_summary()\n",
                                        "\n",
                                        "# Persist outputs to disk.\n",
                                        "analyzer.export_to_csv()\n",
                                        "analyzer.export_to_excel()\n",
                                        "analyzer.export_missing_records()\n",
                                        "\n",
                                        "results[\"column_analysis\"].head()\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "1747dfe2",
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "Channels:\n",
                                                            " - defaults\n",
                                                            " - conda-forge\n",
                                                            "Platform: win-64\n",
                                                            "Collecting package metadata (repodata.json): done\n",
                                                            "Solving environment: done\n",
                                                            "\n",
                                                            "## Package Plan ##\n",
                                                            "\n",
                                                            "  environment location: c:\\Users\\arnav\\OneDrive\\Desktop\\Unimelb\\.conda\n",
                                                            "\n",
                                                            "  added / updated specs:\n",
                                                            "    - openpyxl\n",
                                                            "\n",
                                                            "\n",
                                                            "The following packages will be downloaded:\n",
                                                            "\n",
                                                            "    package                    |            build\n",
                                                            "    ---------------------------|-----------------\n",
                                                            "    et_xmlfile-2.0.0           |  py311haa95532_0          35 KB\n",
                                                            "    openpyxl-3.1.5             |  py311h827c3e9_1         672 KB\n",
                                                            "    ------------------------------------------------------------\n",
                                                            "                                           Total:         707 KB\n",
                                                            "\n",
                                                            "The following NEW packages will be INSTALLED:\n",
                                                            "\n",
                                                            "  et_xmlfile         pkgs/main/win-64::et_xmlfile-2.0.0-py311haa95532_0 \n",
                                                            "  openpyxl           pkgs/main/win-64::openpyxl-3.1.5-py311h827c3e9_1 \n",
                                                            "\n",
                                                            "\n",
                                                            "\n",
                                                            "Downloading and Extracting Packages: ...working...\n",
                                                            "openpyxl-3.1.5       | 672 KB    |            |   0% \n",
                                                            "\n",
                                                            "et_xmlfile-2.0.0     | 35 KB     |            |   0% \u001b[A\n",
                                                            "\n",
                                                            "et_xmlfile-2.0.0     | 35 KB     | ####5      |  45% \u001b[A\n",
                                                            "\n",
                                                            "et_xmlfile-2.0.0     | 35 KB     | ########## | 100% \u001b[A\n",
                                                            "openpyxl-3.1.5       | 672 KB    | 2          |   2% \n",
                                                            "\n",
                                                            "et_xmlfile-2.0.0     | 35 KB     | ########## | 100% \u001b[A\n",
                                                            "openpyxl-3.1.5       | 672 KB    | ######9    |  69% \n",
                                                            "openpyxl-3.1.5       | 672 KB    | ########## | 100% \n",
                                                            "openpyxl-3.1.5       | 672 KB    | ########## | 100% \n",
                                                            "                                                     \n",
                                                            "\n",
                                                            "\n",
                                                            "                                                     \u001b[A done\n",
                                                            "Preparing transaction: done\n",
                                                            "Verifying transaction: done\n",
                                                            "Executing transaction: done\n",
                                                            "\n",
                                                            "Note: you may need to restart the kernel to use updated packages.\n"
                                                  ]
                                        }
                              ],
                              "source": [
                                        "%conda install openpyxl ss"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "f2bac830",
                              "metadata": {},
                              "outputs": [],
                              "source": []
                    }
          ],
          "metadata": {
                    "kernelspec": {
                              "display_name": ".conda",
                              "language": "python",
                              "name": "python3"
                    },
                    "language_info": {
                              "codemirror_mode": {
                                        "name": "ipython",
                                        "version": 3
                              },
                              "file_extension": ".py",
                              "mimetype": "text/x-python",
                              "name": "python",
                              "nbconvert_exporter": "python",
                              "pygments_lexer": "ipython3",
                              "version": "3.11.14"
                    }
          },
          "nbformat": 4,
          "nbformat_minor": 5
}
